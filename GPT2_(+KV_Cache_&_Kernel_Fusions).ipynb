{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghubnerr/machine-learning/blob/main/GPT2_(%2BKV_Cache_%26_Kernel_Fusions).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyk7ugw2xPkA"
      },
      "source": [
        "# Attention Mechanisms and Transformers\n",
        "\n",
        "For the notebook, the main papers we'll be basing our implementation on will be:\n",
        "\n",
        "* [Attention is All You Need](https://arxiv.org/pdf/1706.03762) - Published by Google, researches the attention mechanism for machine translation specifically.\n",
        "* [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) - By OpenAI, the first use of decoder-only LLMs starting GPT-1.\n",
        "* [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) - The second paper by OpenAI in the GPT lineup, realesing GPT-2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YiV7n_pAEK7"
      },
      "source": [
        "`Credits: Gabriel Lucchesi and David Ulloa`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "cirHz1TWxaUH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36574ec0-323b-4875-a5f0-5430ad58c479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/69.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/485.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m481.3/485.4 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.3/231.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.1/344.1 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install datasets transformers -q;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Cl2viQQ0xT86"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Tuple, Union, Dict, Any, List\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from flax.training.train_state import TrainState\n",
        "import datasets\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import optax\n",
        "import jax.random as jr\n",
        "import jax.lax as lax\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ5RLKVqx7O-"
      },
      "source": [
        "### Our Tokenizer: [HF AutoTokenizer](https://huggingface.co/docs/transformers/v4.49.0/en/model_doc/auto#transformers.AutoTokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vu3rFGMfxLdL"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# The tokenizer we use must be the same as our model for compatability reasons\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUmLW82Gxsi6",
        "outputId": "bae4153b-a7a1-4434-8c1c-41aa0501067e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[15496, 11, 616, 3290, 318, 13779]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.encode(\"Hello, my dog is cute\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArWxVOnl7XgQ"
      },
      "source": [
        "## Our Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r72_PQT38QwN"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm,trange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5Uy5HFV6c3U"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import tempfile\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "SCRATCH_PATH = Path(\"\")  # Enter path here\n",
        "DATASET_PATH = SCRATCH_PATH / \"openwebtext.pkl\"\n",
        "\n",
        "if not SCRATCH_PATH.exists():\n",
        "    raise NotImplementedError()\n",
        "\n",
        "# Loading dataset or downloading it if doesn't exist\n",
        "if not DATASET_PATH.exists():\n",
        "    dataset = load_dataset(\"openwebtext\", split=\"train[:10%]\", cache_dir=Path(SCRATCH_PATH / \"hf_cache\"))\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        return {\n",
        "            \"input_ids\": tokenizer(\n",
        "                examples[\"text\"], truncation=True, padding=\"max_length\", max_length=1024\n",
        "            )[\"input_ids\"]\n",
        "        }\n",
        "\n",
        "    dataset = dataset.map(tokenize_function, batched=True)\n",
        "    dataset = dataset.shuffle(seed=42)\n",
        "\n",
        "    with open(DATASET_PATH, \"wb\") as f:\n",
        "        pickle.dump(dataset, f)\n",
        "\n",
        "with open(DATASET_PATH, \"rb\") as f:\n",
        "    dataset = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wf8xhKkWpewZ"
      },
      "outputs": [],
      "source": [
        "# Dataset Helper Functions\n",
        "def batch_dataset(dataset, batch_size):\n",
        "    batch = {\"input_ids\": []}\n",
        "    for example in dataset:\n",
        "        batch[\"input_ids\"].append(example[\"input_ids\"])\n",
        "        if len(batch[\"input_ids\"]) == batch_size:\n",
        "            yield {k: jnp.array(v, dtype=jnp.int32) for k, v in batch.items()}\n",
        "            batch = {\"input_ids\": []}\n",
        "\n",
        "def shard_batch(batch):\n",
        "    batch_size = batch[\"input_ids\"].shape[0]\n",
        "    per_device_batch = batch_size // jax.device_count()\n",
        "    return jax.tree.map(lambda x: x.reshape((jax.device_count(), per_device_batch, -1)), batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R64e_-ATx4JA"
      },
      "source": [
        "## Breaking Attention Into Blocks\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\"\n",
        "       alt=\"Image\" width=\"300\" height=\"400\" />\n",
        "</div>\n",
        "\n",
        "The attention mechanism lies within the transformer architecture, meaning the picture below is the general architecture to how we will build our GPT-2 model. At first, the image looks like a lot - so we'll break down each part step-by-step.\n",
        "\n",
        "* **Positional Embeddings:** Values appended to the embeddings, which are important because the attention architecture doesn't involve position to begin with. You can see how these embeddings looked, plotted, [here](https://en.wikipedia.org/wiki/Softmax_function).\n",
        "* **Multi-Head Attention:** Where the attention mechanism takes place, using the KQV formula below:\n",
        "\n",
        "$$\n",
        "Attention(K, Q, V) = softmax(\\frac{QK^T}{\\sqrt{d}})V\n",
        "$$\n",
        "\n",
        "* Aside from simply applying this equation, our model will split the input embeddings into seperate \"attention\" heads. The splitting allows for attention in each head to focus on different features of our input.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=\"https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png\" alt=\"Image\" width=\"450\" height=\"300\" />\n",
        "</div>\n",
        "\n",
        "* **Add & Norm:** After the attention process, we always have a \"Add & Norm\" block. In code it looks fairly simple, involving [layer normalization](https://arxiv.org/abs/1607.06450) and a [residual connection](https://en.wikipedia.org/wiki/Residual_neural_network): `Output = LayerNorm(X + Attention(X))`\n",
        "* **Feed Foward:** Throughout the transformer, these blocks are MLPs which are simply a sequence of dense layers. Most of the weights (and knowledge learned) in the model are located here.\n",
        "* **Softmax & Linear:** Finally, at the end of our model, we apply a single linear layer and [softmax](https://en.wikipedia.org/wiki/Softmax_function) function to create a probability distribution over what word to predict next.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png\"\n",
        "       alt=\"Image\" width=\"400\" height=\"300\" />\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjyvgNpmyscB"
      },
      "source": [
        "### GPT-2's Configurations\n",
        "\n",
        "Here are some configuration variables that our GPT model will use. Notice, we're using a [dataclass](https://docs.python.org/3/library/dataclasses.html) - which makes defining classes more concise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cvjuzpIPy4cH"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "czgm-yA-yu40"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPT2Config:\n",
        "    vocab_size: int = 50257\n",
        "    max_position_embeddings: int = 1024\n",
        "    n_embd: int = 768\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_inner: int = 3072\n",
        "    layer_norm_epsilon: float = 1e-5\n",
        "    dropout_rate: float = 0.1\n",
        "    max_sequence_length: int = 1024\n",
        "    attn_dropout_rate: float = 0.1\n",
        "    initializer_range: float = 0.02"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtAPnvOXzZqz"
      },
      "source": [
        "### Multi-Head Self-Attention\n",
        "\n",
        "As previously mentioned, the multi-head self attention applies the attention mechanism while splitting into seperate heads. The process is as simple as apportioning different parts of our input to different attention heads in set intervals, defined by `batch_size / n_head`.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=\"https://jalammar.github.io/images/t/transformer_attention_heads_z.png\" alt=\"Image\" width=\"600\" height=\"300\" />\n",
        "</div>\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=\"https://raw.githubusercontent.com/chloeli-15/ARENA_img/main/img/transformer-attn-new-v2.png\" alt=\"Image\" width=\"600\" height=\"300\" />\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UMSXFsjx29o"
      },
      "outputs": [],
      "source": [
        "class MHSelfAttention(nn.Module):\n",
        "    config: GPT2Config\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, hidden_states, deterministic: bool = True):\n",
        "        \"\"\"\n",
        "        hidden_states: [batch_size, sequence_length, n_embd]\n",
        "        \"\"\"\n",
        "        cfg = self.config\n",
        "        batch_size, seq_length, hidden_dim = hidden_states.shape\n",
        "        assert hidden_dim == cfg.n_embd\n",
        "\n",
        "        qkv = nn.Dense(\n",
        "            cfg.n_embd * 3,\n",
        "            use_bias=True,\n",
        "            kernel_init=nn.initializers.normal(cfg.initializer_range),\n",
        "            bias_init=nn.initializers.zeros\n",
        "        )(hidden_states)\n",
        "\n",
        "        qkv = qkv.reshape(batch_size, seq_length, 3, cfg.n_head, cfg.n_embd // cfg.n_head)\n",
        "        q, k, v = jnp.split(qkv, 3, axis=2)\n",
        "        q = q.squeeze(axis=2)  # [batch_size, seq_length, n_head, head_dim]\n",
        "        k = k.squeeze(axis=2)\n",
        "        v = v.squeeze(axis=2)\n",
        "\n",
        "        # Transpose for (batch, head, seq, head_dim)\n",
        "        q = q.transpose((0, 2, 1, 3))\n",
        "        k = k.transpose((0, 2, 1, 3))\n",
        "        v = v.transpose((0, 2, 1, 3))\n",
        "\n",
        "        causal_mask = jnp.tril(jnp.ones((seq_length, seq_length)))\n",
        "        causal_mask = causal_mask.reshape(1, 1, seq_length, seq_length)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        dk = jnp.sqrt(k.shape[-1]).astype(q.dtype)\n",
        "        attn_weights = jnp.matmul(q, k.transpose((0, 1, 3, 2))) / dk\n",
        "\n",
        "        # Apply the mask\n",
        "        attn_weights = jnp.where(causal_mask == 0, -1e10, attn_weights)\n",
        "\n",
        "        attn_probs = nn.softmax(attn_weights, axis=-1)\n",
        "        attn_probs = nn.Dropout(rate=cfg.attn_dropout_rate)(attn_probs, deterministic=deterministic)\n",
        "\n",
        "        # Multiply by value matrix\n",
        "        attn_output = jnp.matmul(attn_probs, v)\n",
        "\n",
        "         # Revert transpose\n",
        "        attn_output = attn_output.transpose((0, 2, 1, 3))\n",
        "\n",
        "        # Combine heads\n",
        "        attn_output = attn_output.reshape(batch_size, seq_length, cfg.n_embd)\n",
        "\n",
        "        # Final linear projection\n",
        "        out = nn.Dense(\n",
        "            cfg.n_embd,\n",
        "            use_bias=True,\n",
        "            kernel_init=nn.initializers.normal(cfg.initializer_range),\n",
        "            bias_init=nn.initializers.zeros\n",
        "        )(attn_output)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT5f8wo43Xgs"
      },
      "source": [
        "### The MLP Block\n",
        "\n",
        "The MLP block, otherwise known as a feed forward network. As previously mentioned, the MLP holds two-thirds of the LLM's weights and is also where \"information\" is stored.\n",
        "\n",
        "Let's say the words \"Micheal Jordan\" is inputted, attention would create the relationship that Michael before Jordan is a significant relationship, but the MLP learns who he is and what achievements he has.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=\"https://raw.githubusercontent.com/chloeli-15/ARENA_img/main/img/transformer-mlp-new-2.png\" alt=\"Image\" width=\"550\" height=\"400\" />\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CW3bGoqu2qAE"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    config: GPT2Config\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, hidden_states, deterministic: bool = True):\n",
        "        cfg = self.config\n",
        "\n",
        "        hidden = nn.Dense(\n",
        "            cfg.n_inner,\n",
        "            kernel_init=nn.initializers.normal(cfg.initializer_range),\n",
        "            bias_init=nn.initializers.zeros\n",
        "        )(hidden_states)\n",
        "        hidden = nn.gelu(hidden)\n",
        "\n",
        "        hidden = nn.Dense(\n",
        "            cfg.n_embd,\n",
        "            kernel_init=nn.initializers.normal(cfg.initializer_range),\n",
        "            bias_init=nn.initializers.zeros\n",
        "        )(hidden)\n",
        "\n",
        "        hidden = nn.Dropout(rate=cfg.dropout_rate)(hidden, deterministic=deterministic)\n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agjDmug_3ZZy"
      },
      "source": [
        "### Transformer Block\n",
        "\n",
        "Finally, we get to the transformer block, which we'll create a list of, for our GPT-2 model. In general, this same transformer block that we'll create can be used to make up different sizes of GPT-2.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=\"https://raw.githubusercontent.com/chloeli-15/ARENA_img/main/img/transformer-block2.png\" alt=\"Image\" width=\"600\" height=\"400\" />\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--Qn3YA82tIp"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    config: GPT2Config\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, hidden_states, deterministic: bool = True):\n",
        "        cfg = self.config\n",
        "\n",
        "        attn_ln = nn.LayerNorm(epsilon=cfg.layer_norm_epsilon)(hidden_states)\n",
        "        attn_out = MHSelfAttention(cfg)(attn_ln, deterministic=deterministic)\n",
        "        attn_out = nn.Dropout(rate=cfg.dropout_rate)(attn_out, deterministic=deterministic)\n",
        "\n",
        "        x = hidden_states + attn_out\n",
        "\n",
        "        mlp_ln = nn.LayerNorm(epsilon=cfg.layer_norm_epsilon)(x)\n",
        "        mlp_out = MLP(cfg)(mlp_ln, deterministic=deterministic)\n",
        "\n",
        "        # Residual\n",
        "        x = x + mlp_out\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-05sxVv3fOP"
      },
      "source": [
        "## Putting GPT-2 Together\n",
        "\n",
        "Finally, let's put it all together...\n",
        "\n",
        "![image](https://www.researchgate.net/publication/373352176/figure/fig1/AS:11431281202501967@1698856108167/GPT-2-model-architecture-The-GPT-2-model-contains-N-Transformer-decoder-blocks-as-shown.ppm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjamvKge3GtF"
      },
      "outputs": [],
      "source": [
        "class GPT2(nn.Module):\n",
        "    config: GPT2Config\n",
        "\n",
        "    def setup(self):\n",
        "        # Embedding shared by input and output\n",
        "        self.wte = nn.Embed(\n",
        "            num_embeddings=self.config.vocab_size,\n",
        "            features=self.config.n_embd,\n",
        "            embedding_init=nn.initializers.normal(self.config.initializer_range)\n",
        "        )\n",
        "        self.wpe = self.param(\n",
        "            \"wpe\",\n",
        "            nn.initializers.normal(self.config.initializer_range),\n",
        "            (self.config.max_position_embeddings, self.config.n_embd)\n",
        "        )\n",
        "        self.blocks = [TransformerBlock(self.config) for _ in range(self.config.n_layer)]\n",
        "        self.ln_f = nn.LayerNorm(epsilon=self.config.layer_norm_epsilon)\n",
        "        self.dropout = nn.Dropout(rate=self.config.dropout_rate)\n",
        "\n",
        "    def __call__(self, input_ids, deterministic=True):\n",
        "        # Input embedding\n",
        "        position_ids = jnp.arange(input_ids.shape[1])[None, :]\n",
        "        token_embeds = self.wte(input_ids)\n",
        "        position_embeds = self.wpe[position_ids, :]\n",
        "        hidden_states = token_embeds + position_embeds\n",
        "        hidden_states = self.dropout(hidden_states, deterministic=deterministic)\n",
        "\n",
        "        # Transformer blocks\n",
        "        for block in self.blocks:\n",
        "            hidden_states = block(hidden_states, deterministic=deterministic)\n",
        "\n",
        "        # Final layer norm\n",
        "        hidden_states = self.ln_f(hidden_states)\n",
        "\n",
        "        # Output logits by tying embeddings\n",
        "        # We transpose the embedding matrix from [vocab_size, n_embd] to [n_embd, vocab_size]\n",
        "        wte_tied = self.wte.embedding.T  # shape: [n_embd, vocab_size]\n",
        "        logits = jnp.einsum(\"bld,dk->blk\", hidden_states, wte_tied)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6iT1si25cop"
      },
      "source": [
        "## Training GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bo9PkqOW5r7V",
        "outputId": "f8a27797-5530-404d-fc80-6ceee3455207"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "124439808 (124.44M)\n"
          ]
        }
      ],
      "source": [
        "config = GPT2Config()\n",
        "\n",
        "model = GPT2(config)\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "dummy_input = jnp.zeros((1, 1024), dtype=jnp.int32)\n",
        "params = model.init(rng, dummy_input)['params']\n",
        "\n",
        "optimizer = optax.adamw(learning_rate=5e-5)\n",
        "state = TrainState.create(apply_fn=model.apply, params=params, tx=optimizer)\n",
        "\n",
        "param_count = sum(p.size for p in jax.tree_util.tree_leaves(params))\n",
        "print(param_count, f\"({param_count / 1e6:.2f}M)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFIbXC4KJP6n"
      },
      "outputs": [],
      "source": [
        "import flax.serialization as flax_serialization\n",
        "\n",
        "@dataclass\n",
        "class TrainingParams:\n",
        "    num_epochs = 1\n",
        "    batch_size = 8\n",
        "    seq_length = 1024\n",
        "\n",
        "# state = jax.device_put_replicated(state, jax.devices())\n",
        "\n",
        "CHECKPOINT_DIR = SCRATCH_PATH / \"checkpoints\"\n",
        "\n",
        "if not CHECKPOINT_DIR.exists():\n",
        "    raise NotImplementedError()\n",
        "\n",
        "def save_model(state, filename):\n",
        "    state_dict = flax.serialization.to_state_dict(state)\n",
        "    with open(CHECKPOINT_DIR / filename, \"wb\") as f:\n",
        "        f.write(flax_serialization.msgpack_serialize(state_dict))\n",
        "\n",
        "def load_model(state, filename):\n",
        "    with open(CHECKPOINT_DIR/ filename, \"rb\") as f:\n",
        "        loaded_state_dict = flax_serialization.msgpack_restore(f.read())\n",
        "    return flax.serialization.from_state_dict(state, loaded_state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4q30s3h5qYP"
      },
      "outputs": [],
      "source": [
        "@jax.pmap\n",
        "def train_step(state, batch):\n",
        "    def loss_fn(params):\n",
        "        logits = state.apply_fn({'params': params}, batch['input_ids'])\n",
        "        loss = jnp.mean(optax.softmax_cross_entropy_with_integer_labels(\n",
        "            logits[:, :-1, :], batch['input_ids'][:, 1:]\n",
        "        ))\n",
        "        return loss\n",
        "\n",
        "    loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
        "    new_state = state.apply_gradients(grads=grads)\n",
        "    return new_state, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czJ2RcEF7-YV"
      },
      "outputs": [],
      "source": [
        "training = TrainingParams()\n",
        "\n",
        "for epoch in range(1, training.num_epochs + 1):\n",
        "    step = 0\n",
        "    progress_bar = tqdm(desc=f\"Epoch {epoch}\", unit=\"batch\")\n",
        "\n",
        "    for batch in batch_dataset(dataset, batch_size=training.batch_size):\n",
        "        batch = shard_batch(batch)  # => (8, 1, 1024)\n",
        "        state, loss = train_step(state, batch)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            progress_bar.set_postfix(loss=loss.mean().item())\n",
        "        step += 1\n",
        "        progress_bar.update(1)\n",
        "\n",
        "        save_model(state, f\"checkpoint_epoch_{epoch}.msgpack\")\n",
        "\n",
        "    print(f\"Epoch {epoch} completed. Loss: {loss.mean()}\")\n",
        "    progress_bar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6P-QGtoJP6o",
        "outputId": "66731836-2ce3-4356-869d-6d4885ece385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wpe: shape (8, 1024, 768)\n",
            "wte.embedding: shape (8, 50257, 768)\n",
            "blocks_0.LayerNorm_0.scale: shape (8, 768)\n",
            "blocks_0.LayerNorm_0.bias: shape (8, 768)\n",
            "blocks_0.MHSelfAttention_0.Dense_0.kernel: shape (8, 768, 2304)\n",
            "blocks_0.MHSelfAttention_0.Dense_0.bias: shape (8, 2304)\n",
            "blocks_0.MHSelfAttention_0.Dense_1.kernel: shape (8, 768, 768)\n",
            "blocks_0.MHSelfAttention_0.Dense_1.bias: shape (8, 768)\n",
            "blocks_0.LayerNorm_1.scale: shape (8, 768)\n",
            "blocks_0.LayerNorm_1.bias: shape (8, 768)\n",
            "blocks_0.MLP_0.Dense_0.kernel: shape (8, 768, 3072)\n",
            "blocks_0.MLP_0.Dense_0.bias: shape (8, 3072)\n",
            "blocks_0.MLP_0.Dense_1.kernel: shape (8, 3072, 768)\n",
            "blocks_0.MLP_0.Dense_1.bias: shape (8, 768)\n",
            "blocks_1.LayerNorm_0.scale: shape (8, 768)\n",
            "blocks_1.LayerNorm_0.bias: shape (8, 768)\n",
            "blocks_1.MHSelfAttention_0.Dense_0.kernel: shape (8, 768, 2304)\n",
            "blocks_1.MHSelfAttention_0.Dense_0.bias: shape (8, 2304)\n",
            "blocks_1.MHSelfAttention_0.Dense_1.kernel: shape (8, 768, 768)\n",
            "blocks_1.MHSelfAttention_0.Dense_1.bias: shape (8, 768)\n",
            "blocks_1.LayerNorm_1.scale: shape (8, 768)\n",
            "blocks_1.LayerNorm_1.bias: shape (8, 768)\n",
            "blocks_1.MLP_0.Dense_0.kernel: shape (8, 768, 3072)\n",
            "blocks_1.MLP_0.Dense_0.bias: shape (8, 3072)\n",
            "blocks_1.MLP_0.Dense_1.kernel: shape (8, 3072, 768)\n",
            "blocks_1.MLP_0.Dense_1.bias: shape (8, 768)\n",
            "blocks_2.LayerNorm_0.scale: shape (8, 768)\n",
            "blocks_2.LayerNorm_0.bias: shape (8, 768)\n",
            "blocks_2.MHSelfAttention_0.Dense_0.kernel: shape (8, 768, 2304)\n",
            "blocks_2.MHSelfAttention_0.Dense_0.bias: shape (8, 2304)\n",
            "blocks_2.MHSelfAttention_0.Dense_1.kernel: shape (8, 768, 768)\n",
            "blocks_2.MHSelfAttention_0.Dense_1.bias: shape (8, 768)\n",
            "blocks_2.LayerNorm_1.scale: shape (8, 768)\n",
            "blocks_2.LayerNorm_1.bias: shape (8, 768)\n",
            "blocks_2.MLP_0.Dense_0.kernel: shape (8, 768, 3072)\n",
            "blocks_2.MLP_0.Dense_0.bias: shape (8, 3072)\n",
            "blocks_2.MLP_0.Dense_1.kernel: shape (8, 3072, 768)\n",
            "blocks_2.MLP_0.Dense_1.bias: shape (8, 768)\n",
            "blocks_3.LayerNorm_0.scale: shape (8, 768)\n",
            "blocks_3.LayerNorm_0.bias: shape (8, 768)\n",
            "blocks_3.MHSelfAttention_0.Dense_0.kernel: shape (8, 768, 2304)\n",
            "blocks_3.MHSelfAttention_0.Dense_0.bias: shape (8, 2304)\n",
            "blocks_3.MHSelfAttention_0.Dense_1.kernel: shape (8, 768, 768)\n",
            "blocks_3.MHSelfAttention_0.Dense_1.bias: shape (8, 768)\n",
            "blocks_3.LayerNorm_1.scale: shape (8, 768)\n",
            "blocks_3.LayerNorm_1.bias: shape (8, 768)\n",
            "blocks_3.MLP_0.Dense_0.kernel: shape (8, 768, 3072)\n",
            "blocks_3.MLP_0.Dense_0.bias: shape (8, 3072)\n",
            "blocks_3.MLP_0.Dense_1.kernel: shape (8, 3072, 768)\n",
            "blocks_3.MLP_0.Dense_1.bias: shape (8, 768)\n",
            "blocks_4.LayerNorm_0.scale: shape (8, 768)\n",
            "blocks_4.LayerNorm_0.bias: shape (8, 768)\n",
            "blocks_4.MHSelfAttention_0.Dense_0.kernel: shape (8, 768, 2304)\n",
            "blocks_4.MHSelfAttention_0.Dense_0.bias: shape (8, 2304)\n",
            "blocks_4.MHSelfAttention_0.Dense_1.kernel: shape (8, 768, 768)\n",
            "blocks_4.MHSelfAttention_0.Dense_1.bias: shape (8, 768)\n",
            "blocks_4.LayerNorm_1.scale: shape (8, 768)\n",
            "blocks_4.LayerNorm_1.bias: shape (8, 768)\n",
            "blocks_4.MLP_0.Dense_0.kernel: shape (8, 768, 3072)\n",
            "blocks_4.MLP_0.Dense_0.bias: shape (8, 3072)\n",
            "blocks_4.MLP_0.Dense_1.kernel: shape (8, 3072, 768)\n",
            "blocks_4.MLP_0.Dense_1.bias: shape (8, 768)\n",
            "blocks_5.LayerNorm_0.scale: shape (8, 768)\n",
            "blocks_5.LayerNorm_0.bias: shape (8, 768)\n",
            "blocks_5.MHSelfAttention_0.Dense_0.kernel: shape (8, 768, 2304)\n",
            "blocks_5.MHSelfAttention_0.Dense_0.bias: shape (8, 2304)\n",
            "blocks_5.MHSelfAttention_0.Dense_1.kernel: shape (8, 768, 768)\n",
            "blocks_5.MHSelfAttention_0.Dense_1.bias: shape (8, 768)\n",
            "blocks_5.LayerNorm_1.scale: shape (8, 768)\n",
            "blocks_5.LayerNorm_1.bias: shape (8, 768)\n",
            "blocks_5.MLP_0.Dense_0.kernel: shape (8, 768, 3072)\n",
            "blocks_5.MLP_0.Dense_0.bias: shape (8, 3072)\n",
            "blocks_5.MLP_0.Dense_1.kernel: shape (8, 3072, 768)\n",
            "blocks_5.MLP_0.Dense_1.bias: shape (8, 768)\n",
            "blocks_6.LayerNorm_0.scale: shape (8, 768)\n",
            "blocks_6.LayerNorm_0.bias: shape (8, 768)\n",
            "blocks_6.MHSelfAttention_0.Dense_0.kernel: shape (8, 768, 2304)\n",
            "blocks_6.MHSelfAttention_0.Dense_0.bias: shape (8, 2304)\n",
            "blocks_6.MHSelfAttention_0.Dense_1.kernel: shape (8, 768, 768)\n",
            "blocks_6.MHSelfAttention_0.Dense_1.bias: shape (8, 768)\n",
            "blocks_6.LayerNorm_1.scale: shape (8, 768)\n",
            "blocks_6.LayerNorm_1.bias: shape (8, 768)\n",
            "blocks_6.MLP_0.Dense_0.kernel: shape (8, 768, 3072)\n",
            "blocks_6.MLP_0.Dense_0.bias: shape (8, 3072)\n",
            "blocks_6.MLP_0.Dense_1.kernel: shape (8, 3072, 768)\n",
            "blocks_6.MLP_0.Dense_1.bias: shape (8, 768)\n",
            "blocks_7.LayerNorm_0.scale: shape (8, 768)\n",
            "blocks_7.LayerNorm_0.bias: shape (8, 768)\n",
            "blocks_7.MHSelfAttention_0.Dense_0.kernel: shape (8, 768, 2304)\n",
            "blocks_7.MHSelfAttention_0.Dense_0.bias: shape (8, 2304)\n",
            "blocks_7.MHSelfAttention_0.Dense_1.kernel: shape (8, 768, 768)\n",
            "blocks_7.MHSelfAttention_0.Dense_1.bias: shape (8, 768)\n",
            "blocks_7.LayerNorm_1.scale: shape (8, 768)\n",
            "blocks_7.LayerNorm_1.bias: shape (8, 768)\n",
            "blocks_7.MLP_0.Dense_0.kernel: shape (8, 768, 3072)\n",
            "blocks_7.MLP_0.Dense_0.bias: shape (8, 3072)\n",
            "blocks_7.MLP_0.Dense_1.kernel: shape (8, 3072, 768)\n",
            "blocks_7.MLP_0.Dense_1.bias: shape (8, 768)\n",
            "blocks_8.LayerNorm_0.scale: shape (8, 768)\n",
            "blocks_8.LayerNorm_0.bias: shape (8, 768)\n",
            "blocks_8.MHSelfAttention_0.Dense_0.kernel: shape (8, 768, 2304)\n",
            "blocks_8.MHSelfAttention_0.Dense_0.bias: shape (8, 2304)\n",
            "blocks_8.MHSelfAttention_0.Dense_1.kernel: shape (8, 768, 768)\n",
            "blocks_8.MHSelfAttention_0.Dense_1.bias: shape (8, 768)\n",
            "blocks_8.LayerNorm_1.scale: shape (8, 768)\n",
            "blocks_8.LayerNorm_1.bias: shape (8, 768)\n",
            "blocks_8.MLP_0.Dense_0.kernel: shape (8, 768, 3072)\n",
            "blocks_8.MLP_0.Dense_0.bias: shape (8, 3072)\n",
            "blocks_8.MLP_0.Dense_1.kernel: shape (8, 3072, 768)\n",
            "blocks_8.MLP_0.Dense_1.bias: shape (8, 768)\n",
            "blocks_9.LayerNorm_0.scale: shape (8, 768)\n",
            "blocks_9.LayerNorm_0.bias: shape (8, 768)\n",
            "blocks_9.MHSelfAttention_0.Dense_0.kernel: shape (8, 768, 2304)\n",
            "blocks_9.MHSelfAttention_0.Dense_0.bias: shape (8, 2304)\n",
            "blocks_9.MHSelfAttention_0.Dense_1.kernel: shape (8, 768, 768)\n",
            "blocks_9.MHSelfAttention_0.Dense_1.bias: shape (8, 768)\n",
            "blocks_9.LayerNorm_1.scale: shape (8, 768)\n",
            "blocks_9.LayerNorm_1.bias: shape (8, 768)\n",
            "blocks_9.MLP_0.Dense_0.kernel: shape (8, 768, 3072)\n",
            "blocks_9.MLP_0.Dense_0.bias: shape (8, 3072)\n",
            "blocks_9.MLP_0.Dense_1.kernel: shape (8, 3072, 768)\n",
            "blocks_9.MLP_0.Dense_1.bias: shape (8, 768)\n",
            "blocks_10.LayerNorm_0.scale: shape (8, 768)\n",
            "blocks_10.LayerNorm_0.bias: shape (8, 768)\n",
            "blocks_10.MHSelfAttention_0.Dense_0.kernel: shape (8, 768, 2304)\n",
            "blocks_10.MHSelfAttention_0.Dense_0.bias: shape (8, 2304)\n",
            "blocks_10.MHSelfAttention_0.Dense_1.kernel: shape (8, 768, 768)\n",
            "blocks_10.MHSelfAttention_0.Dense_1.bias: shape (8, 768)\n",
            "blocks_10.LayerNorm_1.scale: shape (8, 768)\n",
            "blocks_10.LayerNorm_1.bias: shape (8, 768)\n",
            "blocks_10.MLP_0.Dense_0.kernel: shape (8, 768, 3072)\n",
            "blocks_10.MLP_0.Dense_0.bias: shape (8, 3072)\n",
            "blocks_10.MLP_0.Dense_1.kernel: shape (8, 3072, 768)\n",
            "blocks_10.MLP_0.Dense_1.bias: shape (8, 768)\n",
            "blocks_11.LayerNorm_0.scale: shape (8, 768)\n",
            "blocks_11.LayerNorm_0.bias: shape (8, 768)\n",
            "blocks_11.MHSelfAttention_0.Dense_0.kernel: shape (8, 768, 2304)\n",
            "blocks_11.MHSelfAttention_0.Dense_0.bias: shape (8, 2304)\n",
            "blocks_11.MHSelfAttention_0.Dense_1.kernel: shape (8, 768, 768)\n",
            "blocks_11.MHSelfAttention_0.Dense_1.bias: shape (8, 768)\n",
            "blocks_11.LayerNorm_1.scale: shape (8, 768)\n",
            "blocks_11.LayerNorm_1.bias: shape (8, 768)\n",
            "blocks_11.MLP_0.Dense_0.kernel: shape (8, 768, 3072)\n",
            "blocks_11.MLP_0.Dense_0.bias: shape (8, 3072)\n",
            "blocks_11.MLP_0.Dense_1.kernel: shape (8, 3072, 768)\n",
            "blocks_11.MLP_0.Dense_1.bias: shape (8, 768)\n",
            "ln_f.scale: shape (8, 768)\n",
            "ln_f.bias: shape (8, 768)\n"
          ]
        }
      ],
      "source": [
        "def print_param_shapes(params, prefix=\"\"):\n",
        "    if isinstance(params, dict):\n",
        "        for name, param in params.items():\n",
        "            print_param_shapes(param, prefix + name + \".\")\n",
        "    else:\n",
        "        print(f\"{prefix[:-1]}: shape {params.shape}\")\n",
        "\n",
        "loaded_state = load_model(state, f\"checkpoint_epoch_5.msgpack\")\n",
        "print_param_shapes(loaded_state.params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TT42pxnnJP6o"
      },
      "outputs": [],
      "source": [
        "unsharded_params = jax.tree.map(lambda x: x[0] if x.shape[0] == 8 else x, loaded_state.params)\n",
        "\n",
        "unsharded_state = TrainState.create(\n",
        "    apply_fn=model.apply,\n",
        "    params=unsharded_params,\n",
        "    tx=optax.adamw(learning_rate=5e-5)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9u5W2m3JP6o",
        "outputId": "f8224077-e416-462f-f38f-fbede0199d72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "blocks_0.LayerNorm_0.bias: shape (768,)\n",
            "blocks_0.LayerNorm_0.scale: shape (768,)\n",
            "blocks_0.LayerNorm_1.bias: shape (768,)\n",
            "blocks_0.LayerNorm_1.scale: shape (768,)\n",
            "blocks_0.MHSelfAttention_0.Dense_0.bias: shape (2304,)\n",
            "blocks_0.MHSelfAttention_0.Dense_0.kernel: shape (768, 2304)\n",
            "blocks_0.MHSelfAttention_0.Dense_1.bias: shape (768,)\n",
            "blocks_0.MHSelfAttention_0.Dense_1.kernel: shape (768, 768)\n",
            "blocks_0.MLP_0.Dense_0.bias: shape (3072,)\n",
            "blocks_0.MLP_0.Dense_0.kernel: shape (768, 3072)\n",
            "blocks_0.MLP_0.Dense_1.bias: shape (768,)\n",
            "blocks_0.MLP_0.Dense_1.kernel: shape (3072, 768)\n",
            "blocks_1.LayerNorm_0.bias: shape (768,)\n",
            "blocks_1.LayerNorm_0.scale: shape (768,)\n",
            "blocks_1.LayerNorm_1.bias: shape (768,)\n",
            "blocks_1.LayerNorm_1.scale: shape (768,)\n",
            "blocks_1.MHSelfAttention_0.Dense_0.bias: shape (2304,)\n",
            "blocks_1.MHSelfAttention_0.Dense_0.kernel: shape (768, 2304)\n",
            "blocks_1.MHSelfAttention_0.Dense_1.bias: shape (768,)\n",
            "blocks_1.MHSelfAttention_0.Dense_1.kernel: shape (768, 768)\n",
            "blocks_1.MLP_0.Dense_0.bias: shape (3072,)\n",
            "blocks_1.MLP_0.Dense_0.kernel: shape (768, 3072)\n",
            "blocks_1.MLP_0.Dense_1.bias: shape (768,)\n",
            "blocks_1.MLP_0.Dense_1.kernel: shape (3072, 768)\n",
            "blocks_10.LayerNorm_0.bias: shape (768,)\n",
            "blocks_10.LayerNorm_0.scale: shape (768,)\n",
            "blocks_10.LayerNorm_1.bias: shape (768,)\n",
            "blocks_10.LayerNorm_1.scale: shape (768,)\n",
            "blocks_10.MHSelfAttention_0.Dense_0.bias: shape (2304,)\n",
            "blocks_10.MHSelfAttention_0.Dense_0.kernel: shape (768, 2304)\n",
            "blocks_10.MHSelfAttention_0.Dense_1.bias: shape (768,)\n",
            "blocks_10.MHSelfAttention_0.Dense_1.kernel: shape (768, 768)\n",
            "blocks_10.MLP_0.Dense_0.bias: shape (3072,)\n",
            "blocks_10.MLP_0.Dense_0.kernel: shape (768, 3072)\n",
            "blocks_10.MLP_0.Dense_1.bias: shape (768,)\n",
            "blocks_10.MLP_0.Dense_1.kernel: shape (3072, 768)\n",
            "blocks_11.LayerNorm_0.bias: shape (768,)\n",
            "blocks_11.LayerNorm_0.scale: shape (768,)\n",
            "blocks_11.LayerNorm_1.bias: shape (768,)\n",
            "blocks_11.LayerNorm_1.scale: shape (768,)\n",
            "blocks_11.MHSelfAttention_0.Dense_0.bias: shape (2304,)\n",
            "blocks_11.MHSelfAttention_0.Dense_0.kernel: shape (768, 2304)\n",
            "blocks_11.MHSelfAttention_0.Dense_1.bias: shape (768,)\n",
            "blocks_11.MHSelfAttention_0.Dense_1.kernel: shape (768, 768)\n",
            "blocks_11.MLP_0.Dense_0.bias: shape (3072,)\n",
            "blocks_11.MLP_0.Dense_0.kernel: shape (768, 3072)\n",
            "blocks_11.MLP_0.Dense_1.bias: shape (768,)\n",
            "blocks_11.MLP_0.Dense_1.kernel: shape (3072, 768)\n",
            "blocks_2.LayerNorm_0.bias: shape (768,)\n",
            "blocks_2.LayerNorm_0.scale: shape (768,)\n",
            "blocks_2.LayerNorm_1.bias: shape (768,)\n",
            "blocks_2.LayerNorm_1.scale: shape (768,)\n",
            "blocks_2.MHSelfAttention_0.Dense_0.bias: shape (2304,)\n",
            "blocks_2.MHSelfAttention_0.Dense_0.kernel: shape (768, 2304)\n",
            "blocks_2.MHSelfAttention_0.Dense_1.bias: shape (768,)\n",
            "blocks_2.MHSelfAttention_0.Dense_1.kernel: shape (768, 768)\n",
            "blocks_2.MLP_0.Dense_0.bias: shape (3072,)\n",
            "blocks_2.MLP_0.Dense_0.kernel: shape (768, 3072)\n",
            "blocks_2.MLP_0.Dense_1.bias: shape (768,)\n",
            "blocks_2.MLP_0.Dense_1.kernel: shape (3072, 768)\n",
            "blocks_3.LayerNorm_0.bias: shape (768,)\n",
            "blocks_3.LayerNorm_0.scale: shape (768,)\n",
            "blocks_3.LayerNorm_1.bias: shape (768,)\n",
            "blocks_3.LayerNorm_1.scale: shape (768,)\n",
            "blocks_3.MHSelfAttention_0.Dense_0.bias: shape (2304,)\n",
            "blocks_3.MHSelfAttention_0.Dense_0.kernel: shape (768, 2304)\n",
            "blocks_3.MHSelfAttention_0.Dense_1.bias: shape (768,)\n",
            "blocks_3.MHSelfAttention_0.Dense_1.kernel: shape (768, 768)\n",
            "blocks_3.MLP_0.Dense_0.bias: shape (3072,)\n",
            "blocks_3.MLP_0.Dense_0.kernel: shape (768, 3072)\n",
            "blocks_3.MLP_0.Dense_1.bias: shape (768,)\n",
            "blocks_3.MLP_0.Dense_1.kernel: shape (3072, 768)\n",
            "blocks_4.LayerNorm_0.bias: shape (768,)\n",
            "blocks_4.LayerNorm_0.scale: shape (768,)\n",
            "blocks_4.LayerNorm_1.bias: shape (768,)\n",
            "blocks_4.LayerNorm_1.scale: shape (768,)\n",
            "blocks_4.MHSelfAttention_0.Dense_0.bias: shape (2304,)\n",
            "blocks_4.MHSelfAttention_0.Dense_0.kernel: shape (768, 2304)\n",
            "blocks_4.MHSelfAttention_0.Dense_1.bias: shape (768,)\n",
            "blocks_4.MHSelfAttention_0.Dense_1.kernel: shape (768, 768)\n",
            "blocks_4.MLP_0.Dense_0.bias: shape (3072,)\n",
            "blocks_4.MLP_0.Dense_0.kernel: shape (768, 3072)\n",
            "blocks_4.MLP_0.Dense_1.bias: shape (768,)\n",
            "blocks_4.MLP_0.Dense_1.kernel: shape (3072, 768)\n",
            "blocks_5.LayerNorm_0.bias: shape (768,)\n",
            "blocks_5.LayerNorm_0.scale: shape (768,)\n",
            "blocks_5.LayerNorm_1.bias: shape (768,)\n",
            "blocks_5.LayerNorm_1.scale: shape (768,)\n",
            "blocks_5.MHSelfAttention_0.Dense_0.bias: shape (2304,)\n",
            "blocks_5.MHSelfAttention_0.Dense_0.kernel: shape (768, 2304)\n",
            "blocks_5.MHSelfAttention_0.Dense_1.bias: shape (768,)\n",
            "blocks_5.MHSelfAttention_0.Dense_1.kernel: shape (768, 768)\n",
            "blocks_5.MLP_0.Dense_0.bias: shape (3072,)\n",
            "blocks_5.MLP_0.Dense_0.kernel: shape (768, 3072)\n",
            "blocks_5.MLP_0.Dense_1.bias: shape (768,)\n",
            "blocks_5.MLP_0.Dense_1.kernel: shape (3072, 768)\n",
            "blocks_6.LayerNorm_0.bias: shape (768,)\n",
            "blocks_6.LayerNorm_0.scale: shape (768,)\n",
            "blocks_6.LayerNorm_1.bias: shape (768,)\n",
            "blocks_6.LayerNorm_1.scale: shape (768,)\n",
            "blocks_6.MHSelfAttention_0.Dense_0.bias: shape (2304,)\n",
            "blocks_6.MHSelfAttention_0.Dense_0.kernel: shape (768, 2304)\n",
            "blocks_6.MHSelfAttention_0.Dense_1.bias: shape (768,)\n",
            "blocks_6.MHSelfAttention_0.Dense_1.kernel: shape (768, 768)\n",
            "blocks_6.MLP_0.Dense_0.bias: shape (3072,)\n",
            "blocks_6.MLP_0.Dense_0.kernel: shape (768, 3072)\n",
            "blocks_6.MLP_0.Dense_1.bias: shape (768,)\n",
            "blocks_6.MLP_0.Dense_1.kernel: shape (3072, 768)\n",
            "blocks_7.LayerNorm_0.bias: shape (768,)\n",
            "blocks_7.LayerNorm_0.scale: shape (768,)\n",
            "blocks_7.LayerNorm_1.bias: shape (768,)\n",
            "blocks_7.LayerNorm_1.scale: shape (768,)\n",
            "blocks_7.MHSelfAttention_0.Dense_0.bias: shape (2304,)\n",
            "blocks_7.MHSelfAttention_0.Dense_0.kernel: shape (768, 2304)\n",
            "blocks_7.MHSelfAttention_0.Dense_1.bias: shape (768,)\n",
            "blocks_7.MHSelfAttention_0.Dense_1.kernel: shape (768, 768)\n",
            "blocks_7.MLP_0.Dense_0.bias: shape (3072,)\n",
            "blocks_7.MLP_0.Dense_0.kernel: shape (768, 3072)\n",
            "blocks_7.MLP_0.Dense_1.bias: shape (768,)\n",
            "blocks_7.MLP_0.Dense_1.kernel: shape (3072, 768)\n",
            "blocks_8.LayerNorm_0.bias: shape (768,)\n",
            "blocks_8.LayerNorm_0.scale: shape (768,)\n",
            "blocks_8.LayerNorm_1.bias: shape (768,)\n",
            "blocks_8.LayerNorm_1.scale: shape (768,)\n",
            "blocks_8.MHSelfAttention_0.Dense_0.bias: shape (2304,)\n",
            "blocks_8.MHSelfAttention_0.Dense_0.kernel: shape (768, 2304)\n",
            "blocks_8.MHSelfAttention_0.Dense_1.bias: shape (768,)\n",
            "blocks_8.MHSelfAttention_0.Dense_1.kernel: shape (768, 768)\n",
            "blocks_8.MLP_0.Dense_0.bias: shape (3072,)\n",
            "blocks_8.MLP_0.Dense_0.kernel: shape (768, 3072)\n",
            "blocks_8.MLP_0.Dense_1.bias: shape (768,)\n",
            "blocks_8.MLP_0.Dense_1.kernel: shape (3072, 768)\n",
            "blocks_9.LayerNorm_0.bias: shape (768,)\n",
            "blocks_9.LayerNorm_0.scale: shape (768,)\n",
            "blocks_9.LayerNorm_1.bias: shape (768,)\n",
            "blocks_9.LayerNorm_1.scale: shape (768,)\n",
            "blocks_9.MHSelfAttention_0.Dense_0.bias: shape (2304,)\n",
            "blocks_9.MHSelfAttention_0.Dense_0.kernel: shape (768, 2304)\n",
            "blocks_9.MHSelfAttention_0.Dense_1.bias: shape (768,)\n",
            "blocks_9.MHSelfAttention_0.Dense_1.kernel: shape (768, 768)\n",
            "blocks_9.MLP_0.Dense_0.bias: shape (3072,)\n",
            "blocks_9.MLP_0.Dense_0.kernel: shape (768, 3072)\n",
            "blocks_9.MLP_0.Dense_1.bias: shape (768,)\n",
            "blocks_9.MLP_0.Dense_1.kernel: shape (3072, 768)\n",
            "ln_f.bias: shape (768,)\n",
            "ln_f.scale: shape (768,)\n",
            "wpe: shape (1024, 768)\n",
            "wte.embedding: shape (50257, 768)\n"
          ]
        }
      ],
      "source": [
        "print_param_shapes(unsharded_state.params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CifIXKucJP6t"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def infer(params, input_ids):\n",
        "    logits = model.apply({'params': params}, input_ids, deterministic=True)\n",
        "    return logits\n",
        "\n",
        "input_text = (\n",
        "    \"Once upon a time,\"\n",
        ")\n",
        "\n",
        "encoded_input = tokenizer.encode(input_text, return_tensors=\"np\")\n",
        "input_ids = jnp.array(encoded_input)        # shape: (1, seq_length)\n",
        "input_ids = jax.device_put(input_ids)         # ensure it’s on the GPU\n",
        "\n",
        "params_single = jax.device_put(unsharded_state.params)\n",
        "\n",
        "num_tokens_to_generate = 10\n",
        "\n",
        "generated_ids = input_ids\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "temperature = 0.7\n",
        "\n",
        "for _ in range(num_tokens_to_generate):\n",
        "    logits = infer(params_single, generated_ids)\n",
        "    last_token_logits = logits[:, -1, :] / temperature\n",
        "\n",
        "    rng, sub_rng = jax.random.split(rng)\n",
        "    next_token_id = int(jax.random.categorical(sub_rng, last_token_logits[0]))\n",
        "\n",
        "    next_token = jnp.array([[next_token_id]])\n",
        "    generated_ids = jnp.concatenate([generated_ids, next_token], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNrVqN1WJP6t",
        "outputId": "2e428ebd-8c7d-4867-8081-a67db5a1ff00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Once upon a time, when a man could go against the man to the\n"
          ]
        }
      ],
      "source": [
        "generated_text = tokenizer.decode(generated_ids[0].tolist())\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKXWQ2JXpewb"
      },
      "source": [
        "## Let's make this faster\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCWHnYhIpewb"
      },
      "source": [
        "### Implementing KV-Caching\n",
        "<div align=\"center\">\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*4RwWnUm8zaUJmME0RkkUBQ.png\" />\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxwd2RB1pewb"
      },
      "source": [
        "The transparent rows are the only thing we actually want to compute :)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRyDFp6mpewb"
      },
      "source": [
        "$$ \\begin{aligned}\n",
        "&x_K^i \\leftarrow \\text{Concat}\\bigl(x_K^i,\\; t^i \\cdot W_K^i \\bigr),\\\\[1mm]\n",
        "&x_V^i \\leftarrow \\text{Concat}\\bigl(x_V^i,\\; t^i \\cdot W_V^i \\bigr),\\\\[1mm]\n",
        "&t_Q^i = t^i \\cdot W_Q^i,\\\\[1mm]\n",
        "&t_{out}^i = \\mathrm{softmax}\\Bigl(\\frac{t_Q^i \\,(x_K^i)^\\top}{\\sqrt{h}}\\Bigr)\\; x_V^i \\; W_O^i \\;+\\; t^i,\\\\[1mm]\n",
        "&t^{i+1} = \\sigma\\bigl(t_{out}^i \\cdot W_1\\bigr)\\, W_2 \\;+\\; t_{out}^i.\n",
        "\\end{aligned}$$\n",
        "\n",
        "#### **Key Operations in KV Caching**\n",
        "- **During the first step (prompt processing):** Compute and store the full **K** and **V** tensors.\n",
        "- **For subsequent tokens:**\n",
        "  - Retrieve cached **K** and **V** tensors.\n",
        "  - Concatenate the new token's **K** and **V** to the cache.\n",
        "  - Compute attention only on the updated sequence.\n",
        "\n",
        "Now the computation scale goes to $O(T)$ instead of $O(T^2)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "h16rlMR1pewb"
      },
      "outputs": [],
      "source": [
        "class MHSelfAttention(nn.Module):\n",
        "    config: GPT2Config\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(\n",
        "        self,\n",
        "        hidden_states: jnp.ndarray,\n",
        "        deterministic: bool = True,\n",
        "        cached_key: Optional[jnp.ndarray] = None,\n",
        "        cached_value: Optional[jnp.ndarray] = None,\n",
        "    ) -> Tuple[jnp.ndarray, Tuple[jnp.ndarray, jnp.ndarray]]:\n",
        "        \"\"\"\n",
        "        hidden_states: [batch, new_seq_len, n_embd]\n",
        "        cached_key:   [batch, n_heads, prev_seq_len, head_dim] or None\n",
        "        cached_value: [batch, n_heads, prev_seq_len, head_dim] or None\n",
        "        \"\"\"\n",
        "        cfg = self.config\n",
        "        batch_size, new_seq_len, hidden_dim = hidden_states.shape\n",
        "        head_dim = hidden_dim // cfg.n_head\n",
        "\n",
        "        qkv = nn.Dense(\n",
        "            cfg.n_embd * 3,\n",
        "            use_bias=True,\n",
        "            kernel_init=nn.initializers.normal(cfg.initializer_range),\n",
        "            bias_init=nn.initializers.zeros,\n",
        "        )(hidden_states)\n",
        "\n",
        "        qkv = qkv.reshape(batch_size, new_seq_len, 3, cfg.n_head, head_dim)\n",
        "        q, k, v = jnp.split(qkv, 3, axis=2)\n",
        "        q = q.squeeze(axis=2)  # [batch, new_seq_len, n_head, head_dim]\n",
        "        k = k.squeeze(axis=2)\n",
        "        v = v.squeeze(axis=2)\n",
        "\n",
        "        q = q.transpose((0, 2, 1, 3))\n",
        "        k = k.transpose((0, 2, 1, 3))\n",
        "        v = v.transpose((0, 2, 1, 3))\n",
        "\n",
        "        # Concatenate along the sequence dimension\n",
        "        if cached_key is not None and cached_value is not None:\n",
        "            k = jnp.concatenate([cached_key, k], axis=2)  # seq dimension = axis=2\n",
        "            v = jnp.concatenate([cached_value, v], axis=2)\n",
        "\n",
        "        seq_len_q = q.shape[2]   # new_seq_len\n",
        "        seq_len_k = k.shape[2]   # total length so far (prev + new)\n",
        "\n",
        "        base_mask = jnp.tril(jnp.ones((seq_len_k, seq_len_k)))\n",
        "        causal_mask = base_mask[-seq_len_q:, :]\n",
        "        causal_mask = causal_mask[None, None, :, :]\n",
        "\n",
        "        dk = jnp.sqrt(head_dim).astype(q.dtype)\n",
        "        attn_logits = jnp.matmul(q, k.transpose((0, 1, 3, 2))) / dk\n",
        "        attn_logits = jnp.where(causal_mask == 0, -1e10, attn_logits)\n",
        "\n",
        "        attn_weights = nn.softmax(attn_logits, axis=-1)\n",
        "        attn_weights = nn.Dropout(rate=cfg.attn_dropout_rate)(attn_weights, deterministic=deterministic)\n",
        "\n",
        "        attn_output = jnp.matmul(attn_weights, v)\n",
        "        attn_output = attn_output.transpose((0, 2, 1, 3))\n",
        "        attn_output = attn_output.reshape(batch_size, new_seq_len, cfg.n_embd)\n",
        "\n",
        "        out = nn.Dense(\n",
        "            cfg.n_embd,\n",
        "            use_bias=True,\n",
        "            kernel_init=nn.initializers.normal(cfg.initializer_range),\n",
        "            bias_init=nn.initializers.zeros,\n",
        "        )(attn_output)\n",
        "\n",
        "        return out, (k, v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kw7KcHhJpewb"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    config: GPT2Config\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(\n",
        "        self,\n",
        "        hidden_states: jnp.ndarray,\n",
        "        deterministic: bool = True,\n",
        "        cached_key: Optional[jnp.ndarray] = None,\n",
        "        cached_value: Optional[jnp.ndarray] = None\n",
        "    ) -> Tuple[jnp.ndarray, Tuple[jnp.ndarray, jnp.ndarray]]:\n",
        "        cfg = self.config\n",
        "\n",
        "        ln_attn = nn.LayerNorm(epsilon=cfg.layer_norm_epsilon)(hidden_states)\n",
        "        attn_out, (updated_k, updated_v) = MHSelfAttention(cfg)(\n",
        "            ln_attn, deterministic=deterministic,\n",
        "            cached_key=cached_key, cached_value=cached_value\n",
        "        )\n",
        "        attn_out = nn.Dropout(rate=cfg.dropout_rate)(attn_out, deterministic=deterministic)\n",
        "        x = hidden_states + attn_out\n",
        "\n",
        "        ln_mlp = nn.LayerNorm(epsilon=cfg.layer_norm_epsilon)(x)\n",
        "        mlp_out = MLP(cfg)(ln_mlp, deterministic=deterministic)\n",
        "        x = x + mlp_out\n",
        "\n",
        "        return x, (updated_k, updated_v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKuK06oTpewc"
      },
      "outputs": [],
      "source": [
        "class GPT2(nn.Module):\n",
        "    config: GPT2Config\n",
        "\n",
        "    def setup(self):\n",
        "        cfg = self.config\n",
        "        self.wte = nn.Embed(\n",
        "            num_embeddings=cfg.vocab_size,\n",
        "            features=cfg.n_embd,\n",
        "            embedding_init=nn.initializers.normal(cfg.initializer_range),\n",
        "        )\n",
        "        self.wpe = self.param(\n",
        "            \"wpe\",\n",
        "            nn.initializers.normal(cfg.initializer_range),\n",
        "            (cfg.max_position_embeddings, cfg.n_embd),\n",
        "        )\n",
        "        self.blocks = [TransformerBlock(cfg) for _ in range(cfg.n_layer)]\n",
        "        self.ln_f = nn.LayerNorm(epsilon=cfg.layer_norm_epsilon)\n",
        "        self.dropout = nn.Dropout(rate=cfg.dropout_rate)\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        input_ids: jnp.ndarray,\n",
        "        deterministic: bool = True,\n",
        "        past_key_values: Optional[List[Tuple[jnp.ndarray, jnp.ndarray]]] = None,\n",
        "        use_cache: bool = False,\n",
        "    ) -> Union[jnp.ndarray, Tuple[jnp.ndarray, List[Tuple[jnp.ndarray, jnp.ndarray]]]]:\n",
        "        \"\"\"\n",
        "        input_ids: [batch, seq_len] (can be the full prompt or just the new token).\n",
        "        past_key_values: list of (k, v) tuples, each shape [batch, n_heads, prev_seq_len, head_dim]\n",
        "        use_cache: whether to return updated k/v caches\n",
        "        \"\"\"\n",
        "\n",
        "        cfg = self.config\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "\n",
        "        # Determine offset for position IDs if we have a cache:\n",
        "        if past_key_values is not None and past_key_values[0][0] is not None:\n",
        "            offset = past_key_values[0][0].shape[2]  # 'prev_seq_len' from shape [b, n_heads, prev_seq_len, head_dim]\n",
        "        else:\n",
        "            offset = 0\n",
        "\n",
        "        position_ids = jnp.arange(offset, offset + seq_len, dtype=jnp.int32)[None, :]\n",
        "\n",
        "        token_embeds = self.wte(input_ids)  # [batch, seq_len, n_embd]\n",
        "        position_embeds = jnp.take(self.wpe, position_ids, axis=0)  # also [batch, seq_len, n_embd]\n",
        "        hidden_states = token_embeds + position_embeds\n",
        "        hidden_states = self.dropout(hidden_states, deterministic=deterministic)\n",
        "\n",
        "        if past_key_values is None:\n",
        "            past_key_values = [(None, None)] * len(self.blocks)\n",
        "\n",
        "        new_key_values = []\n",
        "        for block, (cached_k, cached_v) in zip(self.blocks, past_key_values):\n",
        "            hidden_states, (updated_k, updated_v) = block(\n",
        "                hidden_states,\n",
        "                deterministic=deterministic,\n",
        "                cached_key=cached_k,\n",
        "                cached_value=cached_v,\n",
        "            )\n",
        "            new_key_values.append((updated_k, updated_v))\n",
        "\n",
        "        hidden_states = self.ln_f(hidden_states)\n",
        "\n",
        "        wte_tied = self.wte.embedding.T  # shape [n_embd, vocab_size]\n",
        "        logits = jnp.einsum(\"bld,dk->blk\", hidden_states, wte_tied)\n",
        "\n",
        "        if use_cache:\n",
        "            return logits, new_key_values\n",
        "        else:\n",
        "            return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q77htQ99pewc"
      },
      "source": [
        "### Dummy Input and Shape Check (Unsharded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1AYlXgypewc",
        "outputId": "8f806fb2-fbd1-4bb8-a592-721cde0867be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logits shape (no cache): (1, 1024, 50257)\n",
            "Logits shape (with cache): (1, 1024, 50257)\n",
            "Layer 0: k shape: (1, 12, 1024, 64), v shape: (1, 12, 1024, 64)\n",
            "Layer 1: k shape: (1, 12, 1024, 64), v shape: (1, 12, 1024, 64)\n",
            "Layer 2: k shape: (1, 12, 1024, 64), v shape: (1, 12, 1024, 64)\n",
            "Layer 3: k shape: (1, 12, 1024, 64), v shape: (1, 12, 1024, 64)\n",
            "Layer 4: k shape: (1, 12, 1024, 64), v shape: (1, 12, 1024, 64)\n",
            "Layer 5: k shape: (1, 12, 1024, 64), v shape: (1, 12, 1024, 64)\n",
            "Layer 6: k shape: (1, 12, 1024, 64), v shape: (1, 12, 1024, 64)\n",
            "Layer 7: k shape: (1, 12, 1024, 64), v shape: (1, 12, 1024, 64)\n",
            "Layer 8: k shape: (1, 12, 1024, 64), v shape: (1, 12, 1024, 64)\n",
            "Layer 9: k shape: (1, 12, 1024, 64), v shape: (1, 12, 1024, 64)\n",
            "Layer 10: k shape: (1, 12, 1024, 64), v shape: (1, 12, 1024, 64)\n",
            "Layer 11: k shape: (1, 12, 1024, 64), v shape: (1, 12, 1024, 64)\n"
          ]
        }
      ],
      "source": [
        "dummy_input = jnp.zeros((1, 1024), dtype=jnp.int32)\n",
        "\n",
        "config = GPT2Config()\n",
        "model = GPT2(config)\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "params = model.init(rng, dummy_input)['params']\n",
        "state = TrainState.create(apply_fn=model.apply, params=params, tx=optimizer)\n",
        "\n",
        "logits = model.apply({'params': params}, dummy_input, deterministic=True, use_cache=False)\n",
        "print(\"Logits shape (no cache):\", logits.shape)\n",
        "\n",
        "logits, new_key_values = model.apply(\n",
        "    {'params': params},\n",
        "    dummy_input,\n",
        "    deterministic=True,\n",
        "    use_cache=True\n",
        ")\n",
        "print(\"Logits shape (with cache):\", logits.shape)\n",
        "\n",
        "for i, (k, v) in enumerate(new_key_values):\n",
        "    print(f\"Layer {i}: k shape: {k.shape}, v shape: {v.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8mf7dNypewc"
      },
      "source": [
        "### Loading a trained model and applying KV Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dK9UGd69pewc"
      },
      "outputs": [],
      "source": [
        "loaded_state = load_model(state, f\"checkpoint_epoch_100_1.msgpack\")\n",
        "unsharded_params = jax.tree.map(lambda x: x[0] if x.shape[0] == 8 else x, loaded_state.params)\n",
        "\n",
        "unsharded_state = TrainState.create(\n",
        "    apply_fn=model.apply,\n",
        "    params=unsharded_params,\n",
        "    tx=optax.adamw(learning_rate=5e-5)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNJ5FtgEpewc",
        "outputId": "7e25769b-905f-4ac3-8e28-15dcecb5b9a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logits shape (with cache): (1, 1024, 50257)\n",
            "Layer 0: k shape: (1, 12, 1024, 64), v shape: (1, 12, 1024, 64)\n",
            "Layer 1: k shape: (1, 12, 1024, 64), v shape: (1, 12, 1024, 64)\n",
            "Layer 2: k shape: (1, 12, 1024, 64), v shape: (1, 12, 1024, 64)\n",
            "Layer 3: k shape: (1, 12, 1024, 64), v shape: (1, 12, 1024, 64)\n",
            "Layer 4: k shape: (1, 12, 1024, 64), v shape: (1, 12, 1024, 64)\n",
            "Layer 5: k shape: (1, 12, 1024, 64), v shape: (1, 12, 1024, 64)\n",
            "Layer 6: k shape: (1, 12, 1024, 64), v shape: (1, 12, 1024, 64)\n",
            "Layer 7: k shape: (1, 12, 1024, 64), v shape: (1, 12, 1024, 64)\n",
            "Layer 8: k shape: (1, 12, 1024, 64), v shape: (1, 12, 1024, 64)\n",
            "Layer 9: k shape: (1, 12, 1024, 64), v shape: (1, 12, 1024, 64)\n",
            "Layer 10: k shape: (1, 12, 1024, 64), v shape: (1, 12, 1024, 64)\n",
            "Layer 11: k shape: (1, 12, 1024, 64), v shape: (1, 12, 1024, 64)\n"
          ]
        }
      ],
      "source": [
        "logits, new_key_values = model.apply(\n",
        "    {'params': unsharded_state.params},\n",
        "    dummy_input,\n",
        "    deterministic=True,\n",
        "    use_cache=True\n",
        ")\n",
        "print(\"Logits shape (with cache):\", logits.shape)\n",
        "for i, (k, v) in enumerate(new_key_values):\n",
        "    print(f\"Layer {i}: k shape: {k.shape}, v shape: {v.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eh7owRxbpewc",
        "outputId": "9e393866-58c7-41de-cc55-ef830614048e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 960/960 [29:36<00:00,  1.85s/it]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated text:\n",
            "Once upon a time, I got asked how many legs a dog had. And I said: ative is a huge amount of people being thought to make anyone think that would be a high school in the world.\n",
            "\n",
            "The whole family known as \"ScAttling,\" said one of the largest people in the world are considering a large number of people to the world's largest and 40,000 people and four of them to the country.\n",
            "\n",
            "There are a lot of people who live here on music or even the country and there are so many people who are looking for a dog who actually really loved and have their children looking for it, but one of them is so as many people want to buy.\n",
            "\n",
            "\"We are both a lot of people who are looking to find out and look at them,\" said one of the biggest causes of the world.\n",
            "\n",
            "\"We stand in the park and we are going to start seeing the problems that we are going to get jobs and start driving that day and to make sure that what that's going to happen.\"\n",
            "\n",
            "The idea is that keeping the people who come from the streets and the people who live on the streets at the end of the day are still being hit.\n",
            "\n",
            "\"You can see where we're going to be doing that people are going to get on here,\" said the statement.\n",
            "\n",
            "\"If you're going to get a place and we'll be able to go to New Zealand, people, and the places that are going to be very low,\" he said.\n",
            "\n",
            "\"The people who have advanced the same way and the people who are going to find a great job and the people who are going to be more likely to be really good about the people,\" he said.\n",
            "\n",
            "About the parents said it was a part of the people they have ever been doing with them, and that they are not going to agree with the friends they have to find a lot of people that are going to be able to go.\n",
            "\n",
            "\"We're taking that idea to make sure that they are going to be very expensive, and we continue to be really good with people.\"\n",
            "\n",
            "In a statement released by the United States that there are two major problems for people to be asked to look for a couple of months.\n",
            "\n",
            "The people who run for their society receive jobs and are not going to sign in their family, but they are not going to be able to go for people.\n",
            "\n",
            "However, in the UK, there are still fewer people with people or the people in terms of changing the work they are doing and they are not able to find out that they need to know that they are not being taken off.\n",
            "\n",
            "\"They are not going to be good enough (we're) for us, they are going to be able to be back and they are going to be just going to look at work,\" the statement said.\n",
            "\n",
            "The people of the same campaign have to go to understanding what they are going to be doing is that they are not very likely to be able to enter their homes.\n",
            "\n",
            "\"You're going to work out there and have a lot of people who are going to be very seriously excited about it,\" the statement said.\n",
            "\n",
            "\"There are always things that such as to live with the people that are going to be able to look at what we can do, and they want us to be a lot of work they can be going to be able to live,\" the statement added.\n",
            "\n",
            "\"The people that are going to be very difficult and they are going to stand in the community and have to get us a lot of people and they are going in life, and they will be going to be able to continue to be able to be done and have some ideas as they are going to announce it.\"\n",
            "\n",
            "The world needs to be highly expensive but a lot of people are not only able to take care of that.\n",
            "\n",
            "Starting from the end of the war, a large group of people in a year are able to get out for a very high school and a lot of people who are still being able to spend work for the key, they are going to go to look for themselves and get back to the same thing they need to go.\n",
            "\n",
            "The job of hosting people who have seen the work that would be built for people to work. In a statement they are going to be getting the same people who have been involved in life and job pets.\n",
            "\n",
            "\"Our work is important to get the right people to go here and we want to be able to get them back and we need to go and sit into the streets, and we have to go to the people and get what they said we can do and what we will have,\" the statement added.\n",
            "\n",
            "The group, first-generation people who have a\n"
          ]
        }
      ],
      "source": [
        "from tqdm import trange\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "@jax.jit\n",
        "def infer(params, input_ids, past_key_values=None):\n",
        "    logits, past_key_values = model.apply(\n",
        "        {'params': params},\n",
        "        input_ids,\n",
        "        deterministic=True,\n",
        "        use_cache=True,\n",
        "        past_key_values=past_key_values\n",
        "    )\n",
        "    return logits, past_key_values\n",
        "\n",
        "prompt_text = \"Once upon a time, I got asked how many legs a dog had. And I said: \"\n",
        "prompt_ids = tokenizer.encode(prompt_text, add_special_tokens=False)\n",
        "prompt_ids = jnp.array(prompt_ids, dtype=jnp.int32)[None, :]  # shape: (1, seq_length)\n",
        "\n",
        "eos_token_id = tokenizer.eos_token_id\n",
        "max_new_tokens = 960\n",
        "generated_tokens = list(np.array(prompt_ids[0]))\n",
        "times = []\n",
        "\n",
        "past_key_values = None\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "temperature = 0.7\n",
        "\n",
        "for i in trange(max_new_tokens):\n",
        "    start_time = time.time()\n",
        "\n",
        "    input_ids = prompt_ids[:, -1:]\n",
        "\n",
        "    logits, past_key_values = infer(unsharded_params, input_ids, past_key_values)\n",
        "    next_token_logits = logits[:, -1, :] / temperature\n",
        "\n",
        "    rng, sub_rng = jax.random.split(rng)\n",
        "    next_token_id = int(jax.random.categorical(sub_rng, next_token_logits[0]))\n",
        "\n",
        "    generated_tokens.append(next_token_id)\n",
        "\n",
        "    if next_token_id == eos_token_id:\n",
        "        break\n",
        "\n",
        "    next_input_ids = jnp.array([[next_token_id]], dtype=jnp.int32)\n",
        "    prompt_ids = jnp.concatenate([prompt_ids, next_input_ids], axis=1)\n",
        "\n",
        "    end_time = time.time()\n",
        "    times.append(end_time - start_time)\n",
        "\n",
        "cumulative_times = np.cumsum(times)\n",
        "token_indices = np.arange(1, len(cumulative_times) + 1)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(token_indices, cumulative_times, marker='o', label='Cumulative Generation Time')\n",
        "plt.xlabel(\"Number of New Tokens Generated\")\n",
        "plt.ylabel(\"Time (seconds)\")\n",
        "plt.title(\"Autoregressive Generation Time (Expected Linear Scaling)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "final_output_text = tokenizer.decode(generated_tokens)\n",
        "print(\"Generated text:\")\n",
        "print(final_output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kernel Optimizations\n"
      ],
      "metadata": {
        "id": "MDKuc2He9hx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Fused Attention Kernels for GPU - [Pallas kernel code](https://github.com/jax-ml/jax/blob/main/jax/experimental/pallas/ops/gpu/attention.py)"
      ],
      "metadata": {
        "id": "sp30X5k29kyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class FusedAttnGPT2Config:\n",
        "    vocab_size: int = 50257\n",
        "    max_position_embeddings: int = 8192\n",
        "    n_embd: int = 2048\n",
        "    n_layer: int = 24\n",
        "    n_head: int = 32\n",
        "    n_inner: int = 8192\n",
        "    layer_norm_epsilon: float = 1e-5\n",
        "    dropout_rate: float = 0.1\n",
        "    max_sequence_length: int = 8192\n",
        "    attn_dropout_rate: float = 0.1\n",
        "    initializer_range: float = 0.02\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AcbDaM3h9gmt"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jax.experimental.pallas.ops.gpu import attention\n",
        "\n",
        "class FusedMHSelfAttention(nn.Module):\n",
        "    config: FusedAttnGPT2Config\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, hidden_states, deterministic=True, cached_key=None, cached_value=None):\n",
        "        cfg = self.config\n",
        "        batch_size, seq_len, hidden_dim = hidden_states.shape\n",
        "        head_dim = hidden_dim // cfg.n_head\n",
        "\n",
        "        qkv = nn.Dense(cfg.n_embd * 3, use_bias=True, name=\"qkv_proj\")(hidden_states)\n",
        "        qkv = qkv.reshape(batch_size, seq_len, 3, cfg.n_head, head_dim)\n",
        "        q, k, v = jnp.split(qkv, 3, axis=2)\n",
        "        q, k, v = [x.squeeze(2).transpose(0, 2, 1, 3) for x in (q, k, v)]\n",
        "\n",
        "        if cached_key is not None and cached_value is not None:\n",
        "            k = jnp.concatenate([cached_key, k], axis=2)\n",
        "            v = jnp.concatenate([cached_value, v], axis=2)\n",
        "\n",
        "        attn_output = attention.mha(\n",
        "            q, k, v,\n",
        "            segment_ids=None,\n",
        "            sm_scale = 1 / (head_dim ** 0.5),\n",
        "            causal=True,\n",
        "            block_q=64,\n",
        "            block_k=64\n",
        "        )\n",
        "\n",
        "        attn_output = attn_output.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, hidden_dim)\n",
        "        out = nn.Dense(cfg.n_embd, use_bias=True, name=\"out_proj\")(attn_output)\n",
        "        return out, (k, v)"
      ],
      "metadata": {
        "id": "9gRsWGvL9mfm"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comparing with standard MHSA"
      ],
      "metadata": {
        "id": "krxyqs839wP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "\n",
        "def benchmark_mha(attention_module, params, hidden_states):\n",
        "    \"\"\"Benchmarks the MHA module with JAX jit compilation.\"\"\"\n",
        "    mha_fn = jax.jit(lambda h: attention_module.apply({'params': params}, h, deterministic=True)[0])\n",
        "    mha_fn(hidden_states).block_until_ready()\n",
        "    return timeit.timeit(lambda: mha_fn(hidden_states).block_until_ready(), number=10) / 10"
      ],
      "metadata": {
        "id": "Ooanop5i9nNn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Running on:\", jax.devices())\n",
        "\n",
        "config = FusedAttnGPT2Config()\n",
        "rng = jax.random.PRNGKey(0)\n",
        "batch_size, seq_len = 1, 4096\n",
        "hidden_states = jax.random.normal(rng, (batch_size, seq_len, config.n_embd))\n",
        "\n",
        "mhsa_model = MHSelfAttention(config)\n",
        "fused_mhsa_model = FusedMHSelfAttention(config)\n",
        "mhsa_params = mhsa_model.init(rng, hidden_states)['params']\n",
        "fused_mhsa_params = fused_mhsa_model.init(rng, hidden_states)['params']\n",
        "\n",
        "mhsa_time = benchmark_mha(mhsa_model, mhsa_params, hidden_states)\n",
        "fused_mhsa_time = benchmark_mha(fused_mhsa_model, fused_mhsa_params, hidden_states)\n",
        "\n",
        "print(f\"Standard MHSA execution time: {mhsa_time:.6f} seconds\")\n",
        "print(f\"Pallas Fused MHSA execution time: {fused_mhsa_time:.6f} seconds\")\n",
        "print(f\"Speedup factor: {mhsa_time / fused_mhsa_time:.2f}x\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uf2UNNqR9zXO",
        "outputId": "fd6e8783-9512-48a5-eeb5-5082a9b3278e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on: [cuda(id=0), cuda(id=1), cuda(id=2), cuda(id=3), cuda(id=4), cuda(id=5), cuda(id=6), cuda(id=7)]\n",
            "Standard MHSA execution time: 0.017768 seconds\n",
            "Pallas Fused MHSA execution time: 0.004653 seconds\n",
            "Speedup factor: 3.82x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Flash Attention Kernels for TPUs - [Pallas kernel code](https://github.com/jax-ml/jax/blob/main/jax/experimental/pallas/ops/tpu/flash_attention.py)"
      ],
      "metadata": {
        "id": "us5rjKm4-PfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax.experimental.pallas.ops.tpu.flash_attention import flash_attention\n",
        "from jax.experimental.pallas.ops.tpu.flash_attention import BlockSizes\n",
        "\n",
        "\n",
        "class FlashAttnMHSelfAttention(nn.Module):\n",
        "    config: FusedAttnGPT2Config\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, hidden_states, deterministic=True, cached_key=None, cached_value=None):\n",
        "        cfg = self.config\n",
        "        batch_size, seq_len, hidden_dim = hidden_states.shape\n",
        "        head_dim = hidden_dim // cfg.n_head\n",
        "\n",
        "        qkv = nn.Dense(cfg.n_embd * 3, use_bias=True, name=\"qkv_proj\")(hidden_states)\n",
        "        qkv = qkv.reshape(batch_size, seq_len, 3, cfg.n_head, head_dim)\n",
        "        q, k, v = jnp.split(qkv, 3, axis=2)\n",
        "        q, k, v = [x.squeeze(2).transpose(0, 2, 1, 3) for x in (q, k, v)]\n",
        "\n",
        "        if cached_key is not None and cached_value is not None:\n",
        "            k = jnp.concatenate([cached_key, k], axis=2)\n",
        "            v = jnp.concatenate([cached_value, v], axis=2)\n",
        "\n",
        "        block_sizes = BlockSizes.get_default(batch_size, cfg.n_head, seq_len, seq_len, head_dim)\n",
        "        attn_output = flash_attention(\n",
        "            q, k, v,\n",
        "            causal=True,\n",
        "            sm_scale=1 / (head_dim ** 0.5),\n",
        "            block_sizes=block_sizes,\n",
        "            debug=False\n",
        "        )\n",
        "\n",
        "        attn_output = attn_output.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, hidden_dim)\n",
        "        out = nn.Dense(cfg.n_embd, use_bias=True, name=\"out_proj\")(attn_output)\n",
        "        return out, (k, v)\n"
      ],
      "metadata": {
        "id": "aSf7pN_y-PG6"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Running on:\", jax.devices())\n",
        "\n",
        "config = FusedAttnGPT2Config()\n",
        "rng = jax.random.PRNGKey(0)\n",
        "batch_size, seq_len = 1, 4096\n",
        "hidden_states = jax.random.normal(rng, (batch_size, seq_len, config.n_embd))\n",
        "\n",
        "mhsa_model = MHSelfAttention(config)\n",
        "flash_attn_mhsa_model = FlashAttnMHSelfAttention(config)\n",
        "mhsa_params = mhsa_model.init(rng, hidden_states)['params']\n",
        "flash_attn_mhsa_params = flash_attn_mhsa_model.init(rng, hidden_states)['params']\n",
        "\n",
        "mhsa_time = benchmark_mha(mhsa_model, mhsa_params, hidden_states)\n",
        "flash_attn_mhsa_time = benchmark_mha(flash_attn_mhsa_model, flash_attn_mhsa_params, hidden_states)\n",
        "\n",
        "print(f\"Standard MHSA execution time: {mhsa_time:.6f} seconds\")\n",
        "print(f\"Pallas Flash Attn MHSA execution time: {flash_attn_mhsa_time:.6f} seconds\")\n",
        "print(f\"Speedup factor: {mhsa_time / flash_attn_mhsa_time:.2f}x\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPCezclD-wPe",
        "outputId": "bce2458b-a87a-47a6-b520-0d330735abac"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]\n",
            "Standard MHSA execution time: 0.030550 seconds\n",
            "Pallas Flash Attn MHSA execution time: 0.029974 seconds\n",
            "Speedup factor: 1.02x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[A good explanation of why the performance is almost identical on TPUs](https://arc.net/l/quote/ifueyccm)"
      ],
      "metadata": {
        "id": "UbjlsJ-ACX8F"
      }
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}